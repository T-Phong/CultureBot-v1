import os
import json
import numpy as np
import faiss
from sentence_transformers import SentenceTransformer
from datasets import load_dataset, load_from_disk
from huggingface_hub import snapshot_download
from typing import List, Dict, Any, Optional
from huggingface_hub import hf_hub_download
from helper import format_metadata_list_to_context

# ==============================================================================
# Há»† THá»NG RAG 1: Sá»¬ Dá»¤NG HUGGING FACE DATASET
# ==============================================================================
class HuggingFaceRAGService:
    _instance: Optional['HuggingFaceRAGService'] = None
    
    # Singleton Pattern
    def __new__(cls):
        if cls._instance is None:
            print("Khá»Ÿi táº¡o HuggingFaceRAGService...")
            cls._instance = super(HuggingFaceRAGService, cls).__new__(cls)
            cls._instance._initialized = False
        return cls._instance

    def __init__(self):
        if self._initialized:
            return
        
        # --- Cáº¤U HÃŒNH ---
        self.MODEL_NAME = "all-MiniLM-L6-v2"
        
        # ID cá»§a Repo trÃªn Hugging Face chá»©a file index vÃ  data
        # Báº¡n cáº§n Ä‘áº£m báº£o Ä‘Ã£ upload file .faiss vÃ  .json lÃªn repo nÃ y (dáº¡ng Dataset hoáº·c Model)
        self.HF_REPO_ID = "synguyen1106/vietnam_heritage_embeddings_v4"
        self.HF_REPO_TYPE = "dataset" # Hoáº·c "model" hoáº·c "space" tÃ¹y nÆ¡i báº¡n Ä‘á»ƒ file
        
        # TÃªn file trÃªn repo HF
        self.FILENAME_INDEX = "heritage.faiss"
        self.FILENAME_META = "metadata.json"
        # self.FILENAME_IDS = "ids.json" # Náº¿u báº¡n gá»™p vÃ o metadata thÃ¬ ko cáº§n file nÃ y
        
        # Load model & Data
        self._load_model()
        self._load_data()
        
        self._initialized = True
        print("âœ… HuggingFaceRAGService Ä‘Ã£ sáºµn sÃ ng.")

    def _load_model(self):
        print(f"ðŸ¤– [HF RAG] Äang táº£i model embedding: {self.MODEL_NAME}...")
        self.model = SentenceTransformer(self.MODEL_NAME)

    def _load_data(self):
        """
        Chiáº¿n lÆ°á»£c:
        1. Cá»‘ gáº¯ng táº£i file index Ä‘Ã£ build sáºµn tá»« Hugging Face (Nhanh, trÃ¡nh lá»—i LFS).
        2. Náº¿u khÃ´ng tÃ¬m tháº¥y file trÃªn HF, fallback vá» viá»‡c táº£i Dataset gá»‘c vÃ  build láº¡i index (Cháº­m hÆ¡n).
        """
        try:
            print(f"â¬‡ï¸ [HF RAG] Äang thá»­ táº£i Index pre-built tá»« HF Hub: {self.HF_REPO_ID}...")
            
            # 1. Táº£i file FAISS Index
            # hf_hub_download sáº½ tá»± xá»­ lÃ½ caching vÃ  LFS pointer
            index_path = hf_hub_download(
                repo_id=self.HF_REPO_ID,
                filename=self.FILENAME_INDEX,
                repo_type=self.HF_REPO_TYPE
            )
            
            # 2. Táº£i file Metadata
            metadata_path = hf_hub_download(
                repo_id=self.HF_REPO_ID,
                filename=self.FILENAME_META,
                repo_type=self.HF_REPO_TYPE
            )

            # 3. Load vÃ o RAM
            print(f"ðŸ“‚ [HF RAG] Äang Ä‘á»c file index tá»«: {index_path}")
            self.index = faiss.read_index(index_path)
            
            with open(metadata_path, "r", encoding="utf-8") as f:
                self.metadata = json.load(f)
                
            print(f"âœ… [HF RAG] Load thÃ nh cÃ´ng tá»« Cache HF! (Items: {self.index.ntotal})")

        except Exception as e:
            print(f"âš ï¸ [HF RAG] KhÃ´ng táº£i Ä‘Æ°á»£c pre-built index ({e}). \nðŸ”„ Chuyá»ƒn sang build tá»« Dataset gá»‘c...")
            self._build_from_dataset()

    def _build_from_dataset(self):
        """
        HÃ m fallback: Táº£i dataset thÃ´ vÃ  build index táº¡i chá»— (Tá»‘n RAM vÃ  CPU lÃºc khá»Ÿi Ä‘á»™ng)
        """
        print("ðŸ’¾ [HF RAG] Äang táº£i dataset vÃ  xÃ¢y dá»±ng FAISS index má»›i...")
        dataset = load_dataset(self.HF_REPO_ID, split="train")
        
        # Chuáº©n bá»‹ vectors
        vectors = np.array(dataset['embedding']).astype("float32")
        
        # Chuáº©n bá»‹ metadata (loáº¡i bá» cá»™t embedding Ä‘á»ƒ nháº¹ RAM)
        self.metadata = [{k: v for k, v in item.items() if k != 'embedding'} for item in dataset]
        
        # Build Index
        d = vectors.shape[1]
        self.index = faiss.IndexFlatL2(d)
        self.index.add(vectors)
        
        print(f"ðŸ”¨ [HF RAG] ÄÃ£ build xong index. Sá»‘ lÆ°á»£ng vector: {self.index.ntotal}")
        
        # Máº¹o: á»ž Ä‘Ã¢y báº¡n cÃ³ thá»ƒ lÆ°u file ra Ä‘Ä©a vÃ  upload ngÆ°á»£c lÃªn HF Ä‘á»ƒ láº§n sau dÃ¹ng cÃ¡ch 1

    def search(self, query: str, k: int = 2) -> List[Dict[str, Any]]:
        # Encode cÃ¢u há»i
        query_vec = self.model.encode([query], convert_to_numpy=True).astype("float32")
        
        # Search FAISS
        distances, indices = self.index.search(query_vec, k)
        
        # Map káº¿t quáº£
        results = []
        for i, idx in enumerate(indices[0]):
            if idx != -1: # Kiá»ƒm tra náº¿u tÃ¬m tháº¥y
                item = {
                    "score": float(distances[0][i]), # Distance cÃ ng nhá» cÃ ng giá»‘ng (vá»›i L2)
                    "metadata": self.metadata[int(idx)]
                }
                results.append(item)
                
        return results
# ==============================================================================
# Há»† THá»NG RAG 2: Sá»¬ Dá»¤NG LOCAL DISK DATASET
# ==============================================================================
class LocalDiskRAGService:
    _instance: Optional['LocalDiskRAGService'] = None

    def __new__(cls):
        if cls._instance is None:
            print("\nKhá»Ÿi táº¡o LocalDiskRAGService...")
            cls._instance = super(LocalDiskRAGService, cls).__new__(cls)
            cls._instance._initialized = False
        return cls._instance

    def __init__(self):
        if self._initialized:
            return
        
        # Cáº¥u hÃ¬nh
        self.MODEL_NAME = 'AITeamVN/Vietnamese_Embedding_v2'
        # Thay Ä‘á»•i tá»« Ä‘Æ°á»ng dáº«n local sang ID cá»§a dataset trÃªn Hugging Face Hub
        self.DATASET_ID = "phongnt251199/Wiki_Culture_Vec"
        self.MIN_CONTENT_LENGTH = 200
        self.CANDIDATE_MULTIPLIER = 5
        
        # Táº£i model vÃ  dá»¯ liá»‡u
        self._load_model()
        self._load_data()
        self._initialized = True
        print("âœ… LocalDiskRAGService Ä‘Ã£ sáºµn sÃ ng.")

    def _load_model(self):
        print(f"ðŸ¤– [Local RAG] Äang táº£i model AI: {self.MODEL_NAME}...")
        self.model = SentenceTransformer(self.MODEL_NAME)

    def _load_data(self):
        print(f"ðŸ’¾ [Local RAG] Äang táº£i dá»¯ liá»‡u tá»« Hugging Face Hub: {self.DATASET_ID}...")
        try:
            # Táº£i toÃ n bá»™ dataset vá» vÃ  láº¥y Ä‘Æ°á»ng dáº«n local
            # Hugging Face Spaces sáº½ tá»± Ä‘á»™ng sá»­ dá»¥ng token trong secrets náº¿u repo lÃ  private
            dataset_path = snapshot_download(repo_id=self.DATASET_ID, repo_type="dataset")
            
            self.dataset = load_from_disk(dataset_path)
            print(f"ðŸ’¾ [Local RAG] Load xong! Tá»•ng sá»‘ dá»¯ liá»‡u: {len(self.dataset)} dÃ²ng.")
            
            print("ðŸ”¨ [Local RAG] Äang kÃ­ch hoáº¡t bá»™ tÃ¬m kiáº¿m (Re-indexing)...")
            self.dataset.add_faiss_index(column="embeddings")
            print("ðŸ”¨ [Local RAG] ÄÃ£ kÃ­ch hoáº¡t xong FAISS Index!")
        except Exception as e:
            print(f"âŒ Lá»—i: KhÃ´ng thá»ƒ táº£i dataset tá»« Hub. Lá»—i: {e}")
            self.dataset = None
            return

    def search(self, query: str, top_k: int = 3) -> List[Dict[str, Any]]:
        if not self.dataset:
            return []
            
        # print(f"\nðŸ”Ž [Local RAG] Äang tÃ¬m: '{query}'")
        # print("-" * 50)

        query_vector = self.model.encode(query)
        candidate_k = top_k * self.CANDIDATE_MULTIPLIER
        scores, samples = self.dataset.get_nearest_examples("embeddings", query_vector, k=candidate_k)

        results = []
        for i in range(len(samples['original_content'])):
            if len(results) >= top_k:
                break
            
            content = samples['original_content'][i]
            if len(content) < self.MIN_CONTENT_LENGTH:
                continue

            score = scores[i]
            metadata = samples['metadata'][i]
            metadata['content'] = content
            
            results.append({
                "metadata": metadata,
                "score": score
            })
            
            # In ra console Ä‘á»ƒ debug nhÆ° hÃ m gá»‘c
            # print(f"Top {len(results)} (Äá»™ sai lá»‡ch: {score:.2f}):")
            # print(f"Ná»™i dung: {content[:200]}...")
            # print("-" * 50)

        if not results:
            print(f"KhÃ´ng tÃ¬m tháº¥y káº¿t quáº£ nÃ o cÃ³ ná»™i dung dÃ i hÆ¡n {self.MIN_CONTENT_LENGTH} kÃ½ tá»±.")
        
        return results

# ==============================================================================
# KHá»žI Táº O SERVICE VÃ€ CUNG Cáº¤P CÃC HÃ€M Gá»C
# ==============================================================================
hf_rag_service = HuggingFaceRAGService()
local_rag_service = LocalDiskRAGService()

def retrieve_context(query: str, k: int = 2) -> str:
    """
    TÃ¬m kiáº¿m ngá»¯ cáº£nh sá»­ dá»¥ng há»‡ thá»‘ng RAG tá»« Hugging Face.
    (Giá»¯ nguyÃªn hÃ m gá»‘c Ä‘á»ƒ tÆ°Æ¡ng thÃ­ch)
    """
    print("\n>>> Sá»­ dá»¥ng há»‡ thá»‘ng RAG 1 (HuggingFace)...")
    results = hf_rag_service.search(query, k)
    return format_metadata_list_to_context(results)

def search_heritage(query: str, top_k: int = 3) -> str:
    """
    TÃ¬m kiáº¿m di sáº£n sá»­ dá»¥ng há»‡ thá»‘ng RAG tá»« á»• Ä‘Ä©a cá»¥c bá»™.
    (Giá»¯ nguyÃªn hÃ m gá»‘c Ä‘á»ƒ tÆ°Æ¡ng thÃ­ch)
    """
    print("\n>>> Sá»­ dá»¥ng há»‡ thá»‘ng RAG 2 (Local Disk)...")
    results = local_rag_service.search(query, top_k)
    return format_metadata_list_to_context(results)